{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Frame Extraction\n",
    "def extract_frames(video_path, frame_rate=1):\n",
    "    print(f\"Extracting frames from {video_path}...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_rate == 0:\n",
    "            frames.append(frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    print(f\"Extracted {len(frames)} frames from {video_path}.\")\n",
    "    return frames\n",
    "\n",
    "# Step 2: Normalization & Resizing\n",
    "def preprocess_frame(frame, size=(224, 224)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    frame = transform(frame)\n",
    "    # print(\"Frame preprocessed.\")\n",
    "    return frame\n",
    "\n",
    "# Step 3: Augmentation (optional)\n",
    "def augment_frames(frames):\n",
    "    print(\"Augmenting frames...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0))\n",
    "    ])\n",
    "    augmented_frames = []\n",
    "    for frame in frames:\n",
    "        frame = transform(frame)\n",
    "        augmented_frames.append(frame)\n",
    "    print(f\"Augmented {len(augmented_frames)} frames.\")\n",
    "    return augmented_frames\n",
    "\n",
    "# Step 4: Label Assignment\n",
    "def get_label_from_filename(filename):\n",
    "    label = filename.split()[1].split('.')[0]\n",
    "    print(f\"Assigned label '{label}' to {filename}.\")\n",
    "    return label\n",
    "\n",
    "# Main Preprocessing Function\n",
    "def preprocess_videos(directory):\n",
    "    print(f\"Preprocessing videos in directory: {directory}\")\n",
    "    video_data = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            video_path = os.path.join(directory, filename)\n",
    "            frames = extract_frames(video_path)\n",
    "            frames = [preprocess_frame(frame) for frame in frames]\n",
    "            frames = augment_frames(frames)  # Optional\n",
    "            label = get_label_from_filename(filename)\n",
    "            video_data.append(frames)\n",
    "            labels.append(label)\n",
    "    print(\"Preprocessing complete.\")\n",
    "    return video_data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "video_directory = 'Data1/Words/'\n",
    "video_data, labels = preprocess_videos(video_directory)\n",
    "\n",
    "# Save preprocessed data for later use\n",
    "torch.save(video_data, 'video_data.pt')\n",
    "torch.save(labels, 'labels.pt')\n",
    "print(\"Data saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output:\n",
    "\n",
    "Preprocessing videos in directory: Data1/Words/\n",
    "Extracting frames from Data1/Words/a0 butcher.mp4...\n",
    "Extracted 58 frames from Data1/Words/a0 butcher.mp4.\n",
    "Augmenting frames...\n",
    "Augmented 58 frames.\n",
    "Assigned label 'butcher' to a0 butcher.mp4.\n",
    "Extracting frames from Data1/Words/a1 aadhar_card.mp4...\n",
    "Extracted 94 frames from Data1/Words/a1 aadhar_card.mp4.\n",
    "Augmenting frames...\n",
    "Augmented 94 frames.\n",
    "Assigned label 'aadhar_card' to a1 aadhar_card.mp4.\n",
    "Extracting frames from Data1/Words/a3 dedication.mp4...\n",
    "Extracted 61 frames from Data1/Words/a3 dedication.mp4.\n",
    "Augmenting frames...\n",
    "Augmented 61 frames.\n",
    "Assigned label 'dedication' to a3 dedication.mp4.\n",
    "Extracting frames from Data1/Words/a2 cinematography.mp4...\n",
    "Extracted 72 frames from Data1/Words/a2 cinematography.mp4.\n",
    "Augmenting frames...\n",
    "Augmented 72 frames.\n",
    "Assigned label 'cinematography' to a2 cinematography.mp4.\n",
    "Preprocessing complete.\n",
    "Data saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Architecture (Isolated Sign Recognition):\n",
    "This model will be trained on your isolated dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SignRecognitionModel, self).__init__()\n",
    "        # Use a pre-trained 2D-CNN (e.g., ResNet) as the feature extractor\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()  # Remove the final fully connected layer\n",
    "        \n",
    "        # LSTM to capture temporal relationships\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"Forward pass started.\")\n",
    "        batch_size, seq_length, c, h, w = x.size()\n",
    "        cnn_out = []\n",
    "        \n",
    "        # Pass each frame through the CNN\n",
    "        for t in range(seq_length):\n",
    "            frame = x[:, t, :, :, :]\n",
    "            cnn_out.append(self.cnn(frame))\n",
    "            print(f\"Processed frame {t+1}/{seq_length} through CNN.\")\n",
    "        \n",
    "        # Stack the CNN outputs and pass through LSTM\n",
    "        cnn_out = torch.stack(cnn_out, dim=1)\n",
    "        print(\"Stacked CNN outputs.\")\n",
    "        lstm_out, _ = self.lstm(cnn_out)\n",
    "        print(\"Passed through LSTM.\")\n",
    "        \n",
    "        # Take the output of the last LSTM cell\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        print(\"Extracted output from the last LSTM cell.\")\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        out = self.fc(lstm_out)\n",
    "        print(\"Passed through the fully connected layer.\")\n",
    "        print(\"Forward pass completed.\")\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "num_classes = 4  # Replace with the actual number of classes\n",
    "model = SignRecognitionModel(num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Assuming you have a DataLoader `dataloader` for your dataset\n",
    "# train_model(model, dataloader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reutrial1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
