{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RAFpfi1UzmOa",
        "0lVwPMjmzqRD",
        "Zr2xL_n004aN"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/F1TutkEeNLArZFEYV9p/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushi-MA/trialreu1/blob/main/sept11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVNQYwprycLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af4b6a0-631b-415d-ee65-d39f21942e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/drive/MyDrive/REU/reuwork/trialreu1\n",
            "/content/drive/MyDrive/REU/reuwork/trialreu1\n",
            "Data1\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.15 protobuf-4.25.5 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# !fusermount -u /content/drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "%cd /content/gdrive/MyDrive/REU/reuwork/trialreu1\n",
        "!pwd\n",
        "!ls\n",
        "\n",
        "!pip install mediapipe\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDD7O8ZSwriH",
        "outputId": "87d2f987-587e-4bc7-819d-d3ced726d519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/REU/reuwork/trialreu1\n",
            "/content/gdrive/MyDrive/REU/reuwork/trialreu1\n",
            "Data1  labels.pt  main.ipynb  README.md  sept7.ipynb  video_data.pt\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.15)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.5)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ekRfMSehwgnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zirDYq1myvWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Initialize MediaPipe Hands and Face Mesh models\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_face_mesh = mp.solutions.face_mesh"
      ],
      "metadata": {
        "id": "jMhta7DeyvZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# initial mediapipe only palma nd fingers"
      ],
      "metadata": {
        "id": "RAFpfi1UzmOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to blackout the background and keep only the hand movements\n",
        "def blackout_background(video_path, output_path):\n",
        "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video details\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    # Video writer to save the processed video\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert the image to RGB as MediaPipe expects this format\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get hand landmarks\n",
        "        result = hands.process(image_rgb)\n",
        "\n",
        "        # Create a blackout frame (all black)\n",
        "        black_frame = np.zeros_like(frame)\n",
        "\n",
        "        if result.multi_hand_landmarks:\n",
        "            for hand_landmarks in result.multi_hand_landmarks:\n",
        "                # Draw the hand landmarks on the black frame\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    black_frame,\n",
        "                    hand_landmarks,\n",
        "                    mp_hands.HAND_CONNECTIONS,\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2)\n",
        "                )\n",
        "\n",
        "        # Write the black_frame with only hands visible to output video\n",
        "        out.write(black_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    hands.close()"
      ],
      "metadata": {
        "id": "--1qM2mEwl2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage: Process the dictionary videos\n",
        "dictionary_videos_path = \"/content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/Words\"  # Replace with actual path\n",
        "blackout_videos_output_path = \"/content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/black1\"  # Replace with actual output folder\n",
        "\n",
        "# Iterate over all dictionary videos and process each\n",
        "for video_file in os.listdir(dictionary_videos_path):\n",
        "    if video_file.endswith(\".mp4\"):  # Process only .mp4 videos (adjust if necessary)\n",
        "        video_path = os.path.join(dictionary_videos_path, video_file)\n",
        "        output_path = os.path.join(blackout_videos_output_path, video_file)\n",
        "        blackout_background(video_path, output_path)\n",
        "\n",
        "print(\"All dictionary videos processed and saved in the /content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/black1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnmkKchUxeu1",
        "outputId": "c298c7ce-7c55-4314-e8b6-508fb2e05661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dictionary videos processed and saved in the /content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/black1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0cZCEm1Mzp9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# estimated hands from wrist to down"
      ],
      "metadata": {
        "id": "0lVwPMjmzqRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extend wrist to elbow approximation\n",
        "def extend_wrist_to_elbow(black_frame, hand_landmarks):\n",
        "    # Get wrist landmark (landmark 0 in MediaPipe's hand model)\n",
        "    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
        "\n",
        "    # Calculate the elbow position by extending from wrist along the vector\n",
        "    # Here we are estimating the elbow to be at a certain distance below the wrist\n",
        "    wrist_x = int(wrist.x * black_frame.shape[1])\n",
        "    wrist_y = int(wrist.y * black_frame.shape[0])\n",
        "\n",
        "    # Arbitrary length to extend for the 'elbow'\n",
        "    length = 100  # You can adjust this based on how long you want the arm to be\n",
        "\n",
        "    # Calculate 'elbow' position\n",
        "    elbow_x = wrist_x\n",
        "    elbow_y = wrist_y + length\n",
        "\n",
        "    # Draw the line from wrist to the 'elbow'\n",
        "    cv2.line(black_frame, (wrist_x, wrist_y), (elbow_x, elbow_y), (255, 255, 255), thickness=2)"
      ],
      "metadata": {
        "id": "DN1pvcTzz7A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to blackout the background and keep only the hand movements\n",
        "def blackout_background(video_path, output_path):\n",
        "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video details\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    # Video writer to save the processed video\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert the image to RGB as MediaPipe expects this format\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get hand landmarks\n",
        "        result = hands.process(image_rgb)\n",
        "\n",
        "        # Create a blackout frame (all black)\n",
        "        black_frame = np.zeros_like(frame)\n",
        "\n",
        "        if result.multi_hand_landmarks:\n",
        "            for hand_landmarks in result.multi_hand_landmarks:\n",
        "                # Draw the hand landmarks on the black frame\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    black_frame,\n",
        "                    hand_landmarks,\n",
        "                    mp_hands.HAND_CONNECTIONS,\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2)\n",
        "                )\n",
        "                # Extend the wrist to elbow\n",
        "                extend_wrist_to_elbow(black_frame, hand_landmarks)\n",
        "\n",
        "        # Write the black_frame with only hands visible to output video\n",
        "        out.write(black_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    hands.close()"
      ],
      "metadata": {
        "id": "lo9V7TQLz23j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: Process the dictionary videos\n",
        "dictionary_videos_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/Words\"  # Replace with actual path\n",
        "blackout_videos_output_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black1\"  # Replace with actual output folder\n",
        "\n",
        "# Iterate over all dictionary videos and process each\n",
        "for video_file in os.listdir(dictionary_videos_path):\n",
        "    if video_file.endswith(\".mp4\"):  # Process only .mp4 videos (adjust if necessary)\n",
        "        video_path = os.path.join(dictionary_videos_path, video_file)\n",
        "        output_path = os.path.join(blackout_videos_output_path, video_file)\n",
        "        blackout_background(video_path, output_path)\n",
        "\n",
        "print(\"All dictionary videos processed and saved in the blackout dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQqI-nJqtevF",
        "outputId": "0f6b4a67-43a2-431b-e1c0-a3d51d190c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dictionary videos processed and saved in the blackout dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IChyeCUEtext"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with face"
      ],
      "metadata": {
        "id": "Zr2xL_n004aN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qMUXKZRe085Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FpGkMQGP0-Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extend wrist to elbow approximation\n",
        "def extend_wrist_to_elbow(black_frame, hand_landmarks):\n",
        "    # Get wrist landmark (landmark 0 in MediaPipe's hand model)\n",
        "    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
        "\n",
        "    # Calculate the elbow position by extending from wrist along the vector\n",
        "    wrist_x = int(wrist.x * black_frame.shape[1])\n",
        "    wrist_y = int(wrist.y * black_frame.shape[0])\n",
        "\n",
        "    # Arbitrary length to extend for the 'elbow'\n",
        "    length = 100  # You can adjust this based on how long you want the arm to be\n",
        "\n",
        "    # Calculate 'elbow' position\n",
        "    elbow_x = wrist_x\n",
        "    elbow_y = wrist_y + length\n",
        "\n",
        "    # Draw the line from wrist to the 'elbow'\n",
        "    cv2.line(black_frame, (wrist_x, wrist_y), (elbow_x, elbow_y), (255, 255, 255), thickness=2)"
      ],
      "metadata": {
        "id": "VP7Bklm91Arj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to blackout the background and keep only hand and face landmarks\n",
        "def blackout_background(video_path, output_path):\n",
        "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video details\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    # Video writer to save the processed video\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert the image to RGB as MediaPipe expects this format\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get hand landmarks\n",
        "        result_hands = hands.process(image_rgb)\n",
        "\n",
        "        # Get face landmarks\n",
        "        result_face = face_mesh.process(image_rgb)\n",
        "\n",
        "        # Create a blackout frame (all black)\n",
        "        black_frame = np.zeros_like(frame)\n",
        "\n",
        "        # Draw hands if found\n",
        "        if result_hands.multi_hand_landmarks:\n",
        "            for hand_landmarks in result_hands.multi_hand_landmarks:\n",
        "                # Draw the hand landmarks on the black frame\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    black_frame,\n",
        "                    hand_landmarks,\n",
        "                    mp_hands.HAND_CONNECTIONS,\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2)\n",
        "                )\n",
        "                # Extend the wrist to elbow\n",
        "                extend_wrist_to_elbow(black_frame, hand_landmarks)\n",
        "\n",
        "        # Draw face if found\n",
        "        if result_face.multi_face_landmarks:\n",
        "            for face_landmarks in result_face.multi_face_landmarks:\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    black_frame,\n",
        "                    face_landmarks,\n",
        "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1, circle_radius=1),\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1)\n",
        "                )\n",
        "\n",
        "        # Write the black_frame with only hands and face visible to output video\n",
        "        out.write(black_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    hands.close()\n",
        "    face_mesh.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CEJa1vsTte0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage: Process the dictionary videos\n",
        "dictionary_videos_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/Words\"  # Replace with actual path\n",
        "blackout_videos_output_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black1\"  # Replace with actual output folder\n",
        "\n",
        "# Iterate over all dictionary videos and process each\n",
        "for video_file in os.listdir(dictionary_videos_path):\n",
        "    if video_file.endswith(\".mp4\"):  # Process only .mp4 videos (adjust if necessary)\n",
        "        video_path = os.path.join(dictionary_videos_path, video_file)\n",
        "        output_path = os.path.join(blackout_videos_output_path, video_file)\n",
        "        blackout_background(video_path, output_path)\n",
        "\n",
        "print(\"All dictionary videos processed and saved in the /content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black1\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfvEWD_5te2u",
        "outputId": "0ee7e035-02c5-403b-93a0-a18a3e67b847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dictionary videos processed and saved in the /content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3XDs9I8Yte5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# frame extraction"
      ],
      "metadata": {
        "id": "N-JNey9d1e8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Frame Extraction\n",
        "def extract_frames(video_path, frame_rate=1):\n",
        "    print(f\"Extracting frames from {video_path}...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % frame_rate == 0:\n",
        "            frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    print(f\"Extracted {len(frames)} frames from {video_path}.\")\n",
        "    return frames\n",
        "\n",
        "# Step 2: Normalization & Resizing\n",
        "def preprocess_frame(frame, size=(224, 224)):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    frame = transform(frame)\n",
        "    # print(\"Frame preprocessed.\")\n",
        "    return frame\n",
        "\n",
        "# Step 3: Augmentation (optional)\n",
        "def augment_frames(frames):\n",
        "    print(\"Augmenting frames...\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0))\n",
        "    ])\n",
        "    augmented_frames = []\n",
        "    for frame in frames:\n",
        "        frame = transform(frame)\n",
        "        augmented_frames.append(frame)\n",
        "    print(f\"Augmented {len(augmented_frames)} frames.\")\n",
        "    return augmented_frames\n",
        "\n",
        "# Step 4: Label Assignment\n",
        "def get_label_from_filename(filename):\n",
        "    label = filename.split()[1].split('.')[0]\n",
        "    print(f\"Assigned label '{label}' to {filename}.\")\n",
        "    return label\n",
        "\n",
        "# Main Preprocessing Function\n",
        "def preprocess_videos(directory):\n",
        "    print(f\"Preprocessing videos in directory: {directory}\")\n",
        "    video_data = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            video_path = os.path.join(directory, filename)\n",
        "            frames = extract_frames(video_path)\n",
        "            frames = [preprocess_frame(frame) for frame in frames]\n",
        "            frames = augment_frames(frames)  # Optional\n",
        "            label = get_label_from_filename(filename)\n",
        "            video_data.append(frames)\n",
        "            labels.append(label)\n",
        "    print(\"Preprocessing complete.\")\n",
        "    return video_data, labels\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Example usage\n",
        "video_directory = 'Data1/black1'\n",
        "video_data, labels = preprocess_videos(video_directory)\n",
        "\n",
        "# Save preprocessed data for later use\n",
        "torch.save(video_data, 'video_data.pt')\n",
        "torch.save(labels, 'labels.pt')\n",
        "print(\"Data saved.\")\n"
      ],
      "metadata": {
        "id": "HXsI5OZhte78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fccabc-7694-4d28-ad95-e7f8b9b55102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing videos in directory: Data1/black1\n",
            "Extracting frames from Data1/black1/a0 butcher.mp4...\n",
            "Extracted 58 frames from Data1/black1/a0 butcher.mp4.\n",
            "Augmenting frames...\n",
            "Augmented 58 frames.\n",
            "Assigned label 'butcher' to a0 butcher.mp4.\n",
            "Extracting frames from Data1/black1/a1 aadhar_card.mp4...\n",
            "Extracted 94 frames from Data1/black1/a1 aadhar_card.mp4.\n",
            "Augmenting frames...\n",
            "Augmented 94 frames.\n",
            "Assigned label 'aadhar_card' to a1 aadhar_card.mp4.\n",
            "Preprocessing complete.\n",
            "Data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CflP_wFgte-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# labelling"
      ],
      "metadata": {
        "id": "lGghiUQa2Her"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a label mapping\n",
        "unique_labels = list(set(labels))\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Convert string labels to numerical labels\n",
        "numerical_labels = [label_to_index[label] for label in labels]\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SignDataset(Dataset):\n",
        "    def __init__(self, video_data, labels, transform=None, max_frames=100):\n",
        "        self.video_data = video_data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.max_frames = max_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = self.video_data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Pad or truncate video to max_frames\n",
        "        if len(video) < self.max_frames:\n",
        "            padding = [torch.zeros_like(video[0])] * (self.max_frames - len(video))\n",
        "            video.extend(padding)\n",
        "        else:\n",
        "            video = video[:self.max_frames]\n",
        "\n",
        "        if self.transform:\n",
        "            video = [self.transform(frame) for frame in video]\n",
        "        video = torch.stack(video)\n",
        "        label = torch.tensor(label, dtype=torch.long)  # Ensure label is a tensor of type long\n",
        "        return video, label"
      ],
      "metadata": {
        "id": "V09VIwMktfCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tu4gUY9X2Kx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WS4WByE5tfEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pMQ6trY9tfHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model architecture"
      ],
      "metadata": {
        "id": "utuKAyal2QBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "class SignRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SignRecognitionModel, self).__init__()\n",
        "        # Use a pre-trained 2D-CNN (e.g., ResNet) as the feature extractor\n",
        "        self.cnn = models.resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # LSTM to capture temporal relationships\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Forward pass started.\")\n",
        "        batch_size, seq_length, c, h, w = x.size()\n",
        "        cnn_out = []\n",
        "\n",
        "        # Pass each frame through the CNN\n",
        "        for t in range(seq_length):\n",
        "            frame = x[:, t, :, :, :]\n",
        "            cnn_out.append(self.cnn(frame))\n",
        "            print(f\"Processed frame {t+1}/{seq_length} through CNN.\")\n",
        "\n",
        "        # Stack the CNN outputs and pass through LSTM\n",
        "        cnn_out = torch.stack(cnn_out, dim=1)\n",
        "        print(\"Stacked CNN outputs.\")\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        print(\"Passed through LSTM.\")\n",
        "\n",
        "        # Take the output of the last LSTM cell\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        print(\"Extracted output from the last LSTM cell.\")\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        out = self.fc(lstm_out)\n",
        "        print(\"Passed through the fully connected layer.\")\n",
        "        print(\"Forward pass completed.\")\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "IxUbAMO5tfKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nf-hQZY1tfMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2z2-iNvCtfQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4uFwuwN2kHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjqnK_Gz2kJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# query processing"
      ],
      "metadata": {
        "id": "QseDg75r2lha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "\n",
        "# Load preprocessed data and labels\n",
        "video_data = torch.load('video_data.pt', weights_only=True)\n",
        "labels = torch.load('labels.pt', weights_only=True)\n",
        "\n",
        "print(\"Data and labels loaded.\")\n",
        "\n",
        "import os\n",
        "\n",
        "# Function to perform exact match\n",
        "def exact_match(query, labels):\n",
        "    if query in labels:\n",
        "        index = labels.index(query)\n",
        "        return video_data[index]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to get BERT embeddings for a word\n",
        "def get_bert_embedding(word):\n",
        "    inputs = tokenizer(word, return_tensors='pt')\n",
        "    outputs = bert_model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Function to find the most similar word using BERT\n",
        "def similar_word_search(query, labels):\n",
        "    query_embedding = get_bert_embedding(query)\n",
        "    label_embeddings = [get_bert_embedding(label) for label in labels]\n",
        "\n",
        "    similarities = [cosine_similarity(query_embedding, label_embedding)[0][0] for label_embedding in label_embeddings]\n",
        "    max_similarity_index = np.argmax(similarities)\n",
        "\n",
        "    if similarities[max_similarity_index] > 0.7:  # Threshold for similarity\n",
        "        return labels[max_similarity_index]\n",
        "    else:\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "spP9bHAd2kMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3b9b3f-a2be-40a6-adba-10d9dcf009f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data and labels loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the labels from the .pt file\n",
        "labels_path = \"labels.pt\"  # Replace with the actual path to labels.pt\n",
        "labels = torch.load(labels_path)\n",
        "\n",
        "# Print the loaded labels\n",
        "print(labels)\n",
        "\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# query = \"butcher\"\n",
        "# labels = [\"chef\", \"cook\", \"carpenter\", \"butcher\"]  # Example dictionary of words\n",
        "\n",
        "query = \"butcher\"\n",
        "\n",
        "video_frames = exact_match(query, labels)\n",
        "\n",
        "if not video_frames:\n",
        "    print(f\"Exact match not found for '{query}'. Searching for similar words...\")\n",
        "    similar_word = similar_word_search(query, labels)\n",
        "    if similar_word:\n",
        "        video_frames = exact_match(similar_word, labels)\n",
        "        print(f\"Similar word found: '{similar_word}', corresponding video frames retrieved.\")\n",
        "    else:\n",
        "        print(\"No similar word found.\")\n",
        "else:\n",
        "    print(f\"Exact match found for '{query}', corresponding video frames retrieved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ_ULVpS2kO6",
        "outputId": "dabe9f9d-b3b5-4d4d-a987-9d31fc86412c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['butcher', 'aadhar_card']\n",
            "Exact match found for 'butcher', corresponding video frames retrieved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-d3e49002f8f1>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  labels = torch.load(labels_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7CYJFnt2kRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "La3s2ex62kTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PIKf7rXhEBiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertTokenizer, BertModel\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import numpy as np\n",
        "\n",
        "# # Load BERT tokenizer and model\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Function to get BERT embeddings for a word\n",
        "# def get_bert_embedding(word):\n",
        "#     inputs = tokenizer(word, return_tensors='pt')\n",
        "#     outputs = bert_model(**inputs)\n",
        "#     return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# # Function to find exact match for the word in the dictionary\n",
        "# def exact_match(query, labels):\n",
        "#     if query in labels:\n",
        "#         # Return corresponding video frames for the exact match\n",
        "#         return query  # Placeholder for actual frame retrieval logic\n",
        "#     return None\n",
        "\n",
        "# # Function to search for similar words using BERT embeddings\n",
        "# def similar_word_search(query, labels):\n",
        "#     query_embedding = get_bert_embedding(query)\n",
        "\n",
        "#     # Find the embeddings for all dictionary labels\n",
        "#     label_embeddings = {label: get_bert_embedding(label) for label in labels}\n",
        "\n",
        "#     # Calculate cosine similarities between the query and each label\n",
        "#     similarities = {label: cosine_similarity(query_embedding, label_embedding)[0][0]\n",
        "#                     for label, label_embedding in label_embeddings.items()}\n",
        "\n",
        "#     # Find the most similar word based on cosine similarity\n",
        "#     similar_word = max(similarities, key=similarities.get)\n",
        "\n",
        "#     # Set a threshold for similarity to ensure the words are closely related\n",
        "#     if similarities[similar_word] > 0.7:  # Adjust threshold if necessary\n",
        "#         return similar_word\n",
        "#     return None\n",
        "\n",
        "# # Example usage\n",
        "# query = \"butcher\"\n",
        "# labels = [\"chef\", \"cook\", \"carpenter\", \"butcher\"]  # Example dictionary of words\n",
        "\n",
        "query = \"butcher\"\n",
        "\n",
        "video_frames = exact_match(query, labels)\n",
        "\n",
        "if not video_frames:\n",
        "    print(f\"Exact match not found for '{query}'. Searching for similar words...\")\n",
        "    similar_word = similar_word_search(query, labels)\n",
        "    if similar_word:\n",
        "        video_frames = exact_match(similar_word, labels)\n",
        "        print(f\"Similar word found: '{similar_word}', corresponding video frames retrieved.\")\n",
        "    else:\n",
        "        print(\"No similar word found.\")\n",
        "else:\n",
        "    print(f\"Exact match found for '{query}', corresponding video frames retrieved.\")\n"
      ],
      "metadata": {
        "id": "VjnH2bNO2kcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_frames(frames, model, max_frames=100):\n",
        "    preprocessed_frames = [preprocess_frame(frame) for frame in frames]\n",
        "\n",
        "    # Pad or truncate frames to max_frames\n",
        "    if len(preprocessed_frames) < max_frames:\n",
        "        padding = [torch.zeros_like(preprocessed_frames[0])] * (max_frames - len(preprocessed_frames))\n",
        "        preprocessed_frames.extend(padding)\n",
        "    else:\n",
        "        preprocessed_frames = preprocessed_frames[:max_frames]\n",
        "\n",
        "    preprocessed_frames = torch.stack(preprocessed_frames).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(preprocessed_frames)\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "FEYhcoiB2ke1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LQOOlbhL3weQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Bkc8TnW3wg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8bH5p7m3wjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gK3WsH2V3wnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature extraction forom retrived video"
      ],
      "metadata": {
        "id": "o4IZ_smJ3xEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "\n",
        "class SignRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SignRecognitionModel, self).__init__()\n",
        "        # Use a pre-trained 2D-CNN (e.g., ResNet) as the feature extractor\n",
        "        self.cnn = models.resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # LSTM to capture temporal relationships\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.size()\n",
        "        cnn_out = []\n",
        "\n",
        "        # Pass each frame through the CNN\n",
        "        for t in range(seq_length):\n",
        "            frame = x[:, t, :, :, :]\n",
        "            cnn_out.append(self.cnn(frame))\n",
        "\n",
        "        # Stack the CNN outputs and pass through LSTM\n",
        "        cnn_out = torch.stack(cnn_out, dim=1)\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "\n",
        "        # Take the output of the last LSTM cell\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "HFL04Htw2khh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uToeV_3R2kj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# import the necessary module for DataLoader\n",
        "from torch.utils.data import DataLoader # import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Prepare Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "max_frames = 100  # Set the maximum number of frames for each video\n",
        "dataset = SignDataset(video_data, numerical_labels, transform=transform, max_frames=max_frames)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "num_classes = len(unique_labels)\n",
        "model = SignRecognitionModel(num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create the directory to save the model if it doesn't exist\n",
        "save_dir = '/content/trialreu1'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training loop with model saving at the end of training\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=2, save_path=None):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Batch {batch_idx}/{len(dataloader)} processed. Current loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save the trained model at the specified path\n",
        "    if save_path:\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# Set the save path to the content/model folder\n",
        "save_path = os.path.join(save_dir, 'sign_recognition_model1.pth')\n",
        "\n",
        "# Train the model and save it\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs=2, save_path=save_path)"
      ],
      "metadata": {
        "id": "-VBr4weI2kmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455ad962-2fe6-419b-c1b5-00d3b2c9cbc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 177MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/2\n",
            "Batch 0/1 processed. Current loss: 0.6929\n",
            "Epoch 1/2 completed. Loss: 0.6929\n",
            "Starting epoch 2/2\n",
            "Batch 0/1 processed. Current loss: 0.6025\n",
            "Epoch 2/2 completed. Loss: 0.6025\n",
            "Model saved to /content/trialreu1/sign_recognition_model1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rwzPGaJT2kqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knn2a_E7GmEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5FWwM4rwJBwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aN4YTt0GGmG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7PPzhJBRD06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Spotting in Continuous Video (Temporal Localization)\n",
        "\n",
        "Summary\n",
        "\n",
        "Input Continuous Video: Extract frames and preprocess them.\n",
        "\n",
        "Feature Extraction: Use the same feature extractor (CNN+LSTM) to extract features from the continuous video.\n",
        "\n",
        "Sliding Window Approach: Slide a window over the continuous video to extract features for each segment.\n",
        "\n",
        "Similarity Search: Compute similarity scores between the segment features and the retrieved video's features.\n",
        "\n",
        "Localization: Predict the time interval in the continuous video where the sign appears."
      ],
      "metadata": {
        "id": "NkWGycE1RHMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# MediaPipe setup\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_face = mp.solutions.face_mesh\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "face_mesh = mp_face.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
        "\n",
        "# Function to blackout background and keep only hands and face\n",
        "def blackout_background_with_hands_and_face(frame):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results_hands = hands.process(frame_rgb)\n",
        "    results_face = face_mesh.process(frame_rgb)\n",
        "\n",
        "    # Create an all-black frame of the same size\n",
        "    black_frame = np.zeros_like(frame)\n",
        "\n",
        "    # Draw hand landmarks on the black frame\n",
        "    if results_hands.multi_hand_landmarks:\n",
        "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
        "            mp.solutions.drawing_utils.draw_landmarks(\n",
        "                black_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "    # Draw face landmarks on the black frame\n",
        "    # if results_face.multi_face_landmarks:\n",
        "    #     for face_landmarks in results_face.multi_face_landmarks:\n",
        "    #         mp.solutions.drawing_utils.draw_landmarks(\n",
        "    #             black_frame, face_landmarks, mp_face.FACEMESH_CONTOURS)\n",
        "\n",
        "    return black_frame\n",
        "\n",
        "# Step 1: Input Continuous Video - Extract frames from the continuous video, apply blackout, and save the video\n",
        "def extract_and_blackout_frames(video_path, output_path, frame_rate=1):\n",
        "    print(f\"Extracting and processing frames from {video_path}...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get frame dimensions\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Get the original video's FPS\n",
        "\n",
        "    # Set up VideoWriter to save the processed frames into a video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % frame_rate == 0:\n",
        "            # Blackout background for the current frame (keep only hands and face)\n",
        "            processed_frame = blackout_background_with_hands_and_face(frame)\n",
        "            out.write(processed_frame)  # Write the processed frame to the video\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Extracted and processed frames saved to {output_path}.\")\n",
        "\n",
        "# Example usage:\n",
        "video_path = '/content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0.mp4'\n",
        "output_path = '/content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0-black.mp4'\n",
        "extract_and_blackout_frames(video_path, output_path, frame_rate=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr1VrBlWRCbi",
        "outputId": "b7615383-f33d-4a22-a089-04d9bf108453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting and processing frames from /content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0.mp4...\n",
            "Extracted and processed frames saved to /content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0-black.mp4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Feature Extraction\n",
        "# Use the same feature extractor (CNN+LSTM) to extract features from the continuous video.\n",
        "\n",
        "def extract_features_from_video(video_path, model, max_frames=100):\n",
        "    frames = extract_frames(video_path)\n",
        "    preprocessed_frames = [preprocess_frame(frame) for frame in frames]\n",
        "\n",
        "    # Pad or truncate frames to max_frames\n",
        "    if len(preprocessed_frames) < max_frames:\n",
        "        padding = [torch.zeros_like(preprocessed_frames[0])] * (max_frames - len(preprocessed_frames))\n",
        "        preprocessed_frames.extend(padding)\n",
        "    else:\n",
        "        preprocessed_frames = preprocessed_frames[:max_frames]\n",
        "\n",
        "    preprocessed_frames = torch.stack(preprocessed_frames).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(preprocessed_frames)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Step 3: Sliding Window Approach\n",
        "# Slide a window over the continuous video to extract features for each segment.\n",
        "def sliding_window_features(video_path, model, window_size=30, stride=15):\n",
        "    frames = extract_frames(video_path)\n",
        "    preprocessed_frames = [preprocess_frame(frame) for frame in frames]\n",
        "    num_frames = len(preprocessed_frames)\n",
        "\n",
        "    window_features = []\n",
        "    for start in range(0, num_frames - window_size + 1, stride):\n",
        "        window = preprocessed_frames[start:start + window_size]\n",
        "        window = torch.stack(window).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model(window)\n",
        "\n",
        "        window_features.append((start, features))\n",
        "\n",
        "    return window_features\n",
        "\n",
        "\n",
        "# Step 4: Similarity Search\n",
        "# Compute similarity scores between the segment features and the retrieved video's features.\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def compute_similarity(retrieved_features, window_features):\n",
        "    similarities = []\n",
        "    for start, features in window_features:\n",
        "        similarity = cosine_similarity(retrieved_features, features)\n",
        "        similarities.append((start, similarity))\n",
        "    return similarities\n",
        "\n",
        "\n",
        "# Step 5: Localization\n",
        "# Predict the time interval in the continuous video where the sign appears based on the similarity scores.\n",
        "\n",
        "def find_best_match(similarities, threshold=0.8):\n",
        "    best_match = None\n",
        "    best_score = -1\n",
        "    for start, similarity in similarities:\n",
        "        score = similarity[0][0]\n",
        "        if score > best_score and score > threshold:\n",
        "            best_score = score\n",
        "            best_match = start\n",
        "    return best_match, best_score\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "continuous_video_path = output_path\n",
        "\n",
        "# The video is now saved as 'blackcont.mp4' with only hands and face visible.\n",
        "\n",
        "# Extract features from the retrieved video frames\n",
        "if video_frames:\n",
        "    retrieved_features = extract_features_from_frames(video_frames, model)\n",
        "    if retrieved_features is None:\n",
        "        print(\"Failed to extract features from the retrieved video frames.\")\n",
        "    else:\n",
        "        # Extract features from the continuous video using sliding window\n",
        "        window_features = sliding_window_features(continuous_video_path, model)\n",
        "\n",
        "        # Compute similarity scores\n",
        "        similarities = compute_similarity(retrieved_features, window_features)\n",
        "\n",
        "        # Find the best match\n",
        "        best_match, best_score = find_best_match(similarities)\n",
        "\n",
        "        if best_match is not None:\n",
        "            print(f\"Sign found at timestamp: {best_match} frames with similarity score: {best_score:.4f}\")\n",
        "        else:\n",
        "            print(\"Sign not found in the continuous video.\")\n",
        "else:\n",
        "    print(\"No video frames retrieved for the query.\")"
      ],
      "metadata": {
        "id": "3WX5BJQFJDXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd65c24-54ef-4332-a2c5-e23191d5e44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting frames from /content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0-black.mp4...\n",
            "Extracted 149 frames from /content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0-black.mp4.\n",
            "Sign found at timestamp: 75 frames with similarity score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6tCXvN3GmKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VzLfhV_pRgfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bs7_f_cMRiOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oAr5zb0iRjsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaYCJJsdQ3i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# MediaPipe setup\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_face = mp.solutions.face_mesh\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "face_mesh = mp_face.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
        "\n",
        "# Function to blackout background and keep only hands and face\n",
        "def blackout_background_with_hands_and_face(frame):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results_hands = hands.process(frame_rgb)\n",
        "    results_face = face_mesh.process(frame_rgb)\n",
        "\n",
        "    # Create an all-black frame of the same size\n",
        "    black_frame = np.zeros_like(frame)\n",
        "\n",
        "    # Draw hand landmarks on the black frame\n",
        "    if results_hands.multi_hand_landmarks:\n",
        "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
        "            mp.solutions.drawing_utils.draw_landmarks(\n",
        "                black_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "    # Draw face landmarks on the black frame\n",
        "    if results_face.multi_face_landmarks:\n",
        "        for face_landmarks in results_face.multi_face_landmarks:\n",
        "            mp.solutions.drawing_utils.draw_landmarks(\n",
        "                black_frame, face_landmarks, mp_face.FACEMESH_CONTOURS)\n",
        "\n",
        "    return black_frame\n",
        "\n",
        "# Step 1: Input Continuous Video - Extract frames from the continuous video, apply blackout, and save the video\n",
        "def extract_and_blackout_frames(video_path, output_path, frame_rate=1):\n",
        "    print(f\"Extracting and processing frames from {video_path}...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get frame dimensions\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Get the original video's FPS\n",
        "\n",
        "    # Set up VideoWriter to save the processed frames into a video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % frame_rate == 0:\n",
        "            # Blackout background for the current frame (keep only hands and face)\n",
        "            processed_frame = blackout_background_with_hands_and_face(frame)\n",
        "            out.write(processed_frame)  # Write the processed frame to the video\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Extracted and processed frames saved to {output_path}.\")\n",
        "\n",
        "# Example usage:\n",
        "video_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/v0.mp4\"\n",
        "output_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/test/v0changed.mp4\"\n",
        "extract_and_blackout_frames(video_path, output_path, frame_rate=1)\n",
        "\n",
        "# The video is now saved as 'blackcont.mp4' with only hands and face visible.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_-LbkxhQ3mc",
        "outputId": "69b633a6-3da9-43eb-d9f3-4a6d49e0f98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting and processing frames from /content/drive/MyDrive/REU/reuwork/trialreu1/Data1/v0.mp4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted and processed frames saved to /content/drive/MyDrive/REU/reuwork/trialreu1/Data1/test/v0changed.mp4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def extract_and_save_frames(video_path, output_folder, num_frames=190):\n",
        "    # Ensure the output folder exists\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Handle the case where the video has fewer frames than requested\n",
        "    if total_frames < num_frames:\n",
        "        print(f\"Warning: The video has only {total_frames} frames, but {num_frames} frames were requested.\")\n",
        "        num_frames = total_frames  # Extract all available frames\n",
        "\n",
        "    # Calculate the interval between frames to extract\n",
        "    frame_interval = max(1, total_frames // num_frames)\n",
        "\n",
        "    # Extract and save frames\n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "    while cap.isOpened() and saved_count < num_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Save only the frame at the specified interval\n",
        "        if frame_count % frame_interval == 0:\n",
        "            frame_name = os.path.join(output_folder, f\"frame_{saved_count + 1:04d}.jpg\")\n",
        "            cv2.imwrite(frame_name, frame)\n",
        "            saved_count += 1\n",
        "            print(f\"Saved frame {saved_count} at frame count {frame_count}\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Saved {saved_count} frames to {output_folder}\")\n",
        "\n",
        "# Example usage\n",
        "video_path = '/content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0-black.mp4'\n",
        "output_folder = '/content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0-black-frames'\n",
        "extract_and_save_frames(video_path, output_folder, num_frames=190)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs_tFbKOo_5i",
        "outputId": "abb03a56-33d3-4153-8981-5d7229076b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: The video has only 149 frames, but 190 frames were requested.\n",
            "Saved frame 1 at frame count 0\n",
            "Saved frame 2 at frame count 1\n",
            "Saved frame 3 at frame count 2\n",
            "Saved frame 4 at frame count 3\n",
            "Saved frame 5 at frame count 4\n",
            "Saved frame 6 at frame count 5\n",
            "Saved frame 7 at frame count 6\n",
            "Saved frame 8 at frame count 7\n",
            "Saved frame 9 at frame count 8\n",
            "Saved frame 10 at frame count 9\n",
            "Saved frame 11 at frame count 10\n",
            "Saved frame 12 at frame count 11\n",
            "Saved frame 13 at frame count 12\n",
            "Saved frame 14 at frame count 13\n",
            "Saved frame 15 at frame count 14\n",
            "Saved frame 16 at frame count 15\n",
            "Saved frame 17 at frame count 16\n",
            "Saved frame 18 at frame count 17\n",
            "Saved frame 19 at frame count 18\n",
            "Saved frame 20 at frame count 19\n",
            "Saved frame 21 at frame count 20\n",
            "Saved frame 22 at frame count 21\n",
            "Saved frame 23 at frame count 22\n",
            "Saved frame 24 at frame count 23\n",
            "Saved frame 25 at frame count 24\n",
            "Saved frame 26 at frame count 25\n",
            "Saved frame 27 at frame count 26\n",
            "Saved frame 28 at frame count 27\n",
            "Saved frame 29 at frame count 28\n",
            "Saved frame 30 at frame count 29\n",
            "Saved frame 31 at frame count 30\n",
            "Saved frame 32 at frame count 31\n",
            "Saved frame 33 at frame count 32\n",
            "Saved frame 34 at frame count 33\n",
            "Saved frame 35 at frame count 34\n",
            "Saved frame 36 at frame count 35\n",
            "Saved frame 37 at frame count 36\n",
            "Saved frame 38 at frame count 37\n",
            "Saved frame 39 at frame count 38\n",
            "Saved frame 40 at frame count 39\n",
            "Saved frame 41 at frame count 40\n",
            "Saved frame 42 at frame count 41\n",
            "Saved frame 43 at frame count 42\n",
            "Saved frame 44 at frame count 43\n",
            "Saved frame 45 at frame count 44\n",
            "Saved frame 46 at frame count 45\n",
            "Saved frame 47 at frame count 46\n",
            "Saved frame 48 at frame count 47\n",
            "Saved frame 49 at frame count 48\n",
            "Saved frame 50 at frame count 49\n",
            "Saved frame 51 at frame count 50\n",
            "Saved frame 52 at frame count 51\n",
            "Saved frame 53 at frame count 52\n",
            "Saved frame 54 at frame count 53\n",
            "Saved frame 55 at frame count 54\n",
            "Saved frame 56 at frame count 55\n",
            "Saved frame 57 at frame count 56\n",
            "Saved frame 58 at frame count 57\n",
            "Saved frame 59 at frame count 58\n",
            "Saved frame 60 at frame count 59\n",
            "Saved frame 61 at frame count 60\n",
            "Saved frame 62 at frame count 61\n",
            "Saved frame 63 at frame count 62\n",
            "Saved frame 64 at frame count 63\n",
            "Saved frame 65 at frame count 64\n",
            "Saved frame 66 at frame count 65\n",
            "Saved frame 67 at frame count 66\n",
            "Saved frame 68 at frame count 67\n",
            "Saved frame 69 at frame count 68\n",
            "Saved frame 70 at frame count 69\n",
            "Saved frame 71 at frame count 70\n",
            "Saved frame 72 at frame count 71\n",
            "Saved frame 73 at frame count 72\n",
            "Saved frame 74 at frame count 73\n",
            "Saved frame 75 at frame count 74\n",
            "Saved frame 76 at frame count 75\n",
            "Saved frame 77 at frame count 76\n",
            "Saved frame 78 at frame count 77\n",
            "Saved frame 79 at frame count 78\n",
            "Saved frame 80 at frame count 79\n",
            "Saved frame 81 at frame count 80\n",
            "Saved frame 82 at frame count 81\n",
            "Saved frame 83 at frame count 82\n",
            "Saved frame 84 at frame count 83\n",
            "Saved frame 85 at frame count 84\n",
            "Saved frame 86 at frame count 85\n",
            "Saved frame 87 at frame count 86\n",
            "Saved frame 88 at frame count 87\n",
            "Saved frame 89 at frame count 88\n",
            "Saved frame 90 at frame count 89\n",
            "Saved frame 91 at frame count 90\n",
            "Saved frame 92 at frame count 91\n",
            "Saved frame 93 at frame count 92\n",
            "Saved frame 94 at frame count 93\n",
            "Saved frame 95 at frame count 94\n",
            "Saved frame 96 at frame count 95\n",
            "Saved frame 97 at frame count 96\n",
            "Saved frame 98 at frame count 97\n",
            "Saved frame 99 at frame count 98\n",
            "Saved frame 100 at frame count 99\n",
            "Saved frame 101 at frame count 100\n",
            "Saved frame 102 at frame count 101\n",
            "Saved frame 103 at frame count 102\n",
            "Saved frame 104 at frame count 103\n",
            "Saved frame 105 at frame count 104\n",
            "Saved frame 106 at frame count 105\n",
            "Saved frame 107 at frame count 106\n",
            "Saved frame 108 at frame count 107\n",
            "Saved frame 109 at frame count 108\n",
            "Saved frame 110 at frame count 109\n",
            "Saved frame 111 at frame count 110\n",
            "Saved frame 112 at frame count 111\n",
            "Saved frame 113 at frame count 112\n",
            "Saved frame 114 at frame count 113\n",
            "Saved frame 115 at frame count 114\n",
            "Saved frame 116 at frame count 115\n",
            "Saved frame 117 at frame count 116\n",
            "Saved frame 118 at frame count 117\n",
            "Saved frame 119 at frame count 118\n",
            "Saved frame 120 at frame count 119\n",
            "Saved frame 121 at frame count 120\n",
            "Saved frame 122 at frame count 121\n",
            "Saved frame 123 at frame count 122\n",
            "Saved frame 124 at frame count 123\n",
            "Saved frame 125 at frame count 124\n",
            "Saved frame 126 at frame count 125\n",
            "Saved frame 127 at frame count 126\n",
            "Saved frame 128 at frame count 127\n",
            "Saved frame 129 at frame count 128\n",
            "Saved frame 130 at frame count 129\n",
            "Saved frame 131 at frame count 130\n",
            "Saved frame 132 at frame count 131\n",
            "Saved frame 133 at frame count 132\n",
            "Saved frame 134 at frame count 133\n",
            "Saved frame 135 at frame count 134\n",
            "Saved frame 136 at frame count 135\n",
            "Saved frame 137 at frame count 136\n",
            "Saved frame 138 at frame count 137\n",
            "Saved frame 139 at frame count 138\n",
            "Saved frame 140 at frame count 139\n",
            "Saved frame 141 at frame count 140\n",
            "Saved frame 142 at frame count 141\n",
            "Saved frame 143 at frame count 142\n",
            "Saved frame 144 at frame count 143\n",
            "Saved frame 145 at frame count 144\n",
            "Saved frame 146 at frame count 145\n",
            "Saved frame 147 at frame count 146\n",
            "Saved frame 148 at frame count 147\n",
            "Saved frame 149 at frame count 148\n",
            "Saved 149 frames to /content/gdrive/MyDrive/REU/reuwork/trialreu1/Data1/v0-black-frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JbErUYdxtSkn"
      }
    }
  ]
}