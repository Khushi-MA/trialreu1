{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RAFpfi1UzmOa",
        "0lVwPMjmzqRD",
        "Zr2xL_n004aN"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlBBHczbaN9+sExks2dCQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushi-MA/trialreu1/blob/main/sept11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yVNQYwprycLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3cc97f5-3a79-4119-8979-323a2468116e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/REU/reuwork/trialreu1\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GtMGA_rtesu",
        "outputId": "b8874557-bd91-4e9f-9e11-c81081b8260e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/REU/reuwork/trialreu1\n",
            "/content/drive/MyDrive/REU/reuwork/trialreu1\n",
            "Data1  labels.pt  main.ipynb  README.md  sept7.ipynb  video_data.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaUe_2UFwIYJ",
        "outputId": "3432ae98-19a6-4a42-aa43-0d3672d85d07"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Downloading mediapipe-0.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.0-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.16.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.15 protobuf-4.25.5 sounddevice-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MediaPipe Hands model\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils"
      ],
      "metadata": {
        "id": "ekRfMSehwgnW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# initial mediapipe only palma nd fingers"
      ],
      "metadata": {
        "id": "RAFpfi1UzmOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function to blackout the background and keep only the hand movements\n",
        "# def blackout_background(video_path, output_path):\n",
        "#     hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "#     cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "#     # Get video details\n",
        "#     frame_width = int(cap.get(3))\n",
        "#     frame_height = int(cap.get(4))\n",
        "#     fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "#     # Video writer to save the processed video\n",
        "#     out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "#     while cap.isOpened():\n",
        "#         ret, frame = cap.read()\n",
        "#         if not ret:\n",
        "#             break\n",
        "\n",
        "#         # Convert the image to RGB as MediaPipe expects this format\n",
        "#         image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "#         # Get hand landmarks\n",
        "#         result = hands.process(image_rgb)\n",
        "\n",
        "#         # Create a blackout frame (all black)\n",
        "#         black_frame = np.zeros_like(frame)\n",
        "\n",
        "#         if result.multi_hand_landmarks:\n",
        "#             for hand_landmarks in result.multi_hand_landmarks:\n",
        "#                 # Draw the hand landmarks on the black frame\n",
        "#                 mp_drawing.draw_landmarks(\n",
        "#                     black_frame,\n",
        "#                     hand_landmarks,\n",
        "#                     mp_hands.HAND_CONNECTIONS,\n",
        "#                     mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2)\n",
        "#                 )\n",
        "\n",
        "#         # Write the black_frame with only hands visible to output video\n",
        "#         out.write(black_frame)\n",
        "\n",
        "#     cap.release()\n",
        "#     out.release()\n",
        "#     hands.close()"
      ],
      "metadata": {
        "id": "--1qM2mEwl2Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0cZCEm1Mzp9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# estimated hands from wrist to down"
      ],
      "metadata": {
        "id": "0lVwPMjmzqRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extend wrist to elbow approximation\n",
        "def extend_wrist_to_elbow(black_frame, hand_landmarks):\n",
        "    # Get wrist landmark (landmark 0 in MediaPipe's hand model)\n",
        "    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
        "\n",
        "    # Calculate the elbow position by extending from wrist along the vector\n",
        "    # Here we are estimating the elbow to be at a certain distance below the wrist\n",
        "    wrist_x = int(wrist.x * black_frame.shape[1])\n",
        "    wrist_y = int(wrist.y * black_frame.shape[0])\n",
        "\n",
        "    # Arbitrary length to extend for the 'elbow'\n",
        "    length = 100  # You can adjust this based on how long you want the arm to be\n",
        "\n",
        "    # Calculate 'elbow' position\n",
        "    elbow_x = wrist_x\n",
        "    elbow_y = wrist_y + length\n",
        "\n",
        "    # Draw the line from wrist to the 'elbow'\n",
        "    cv2.line(black_frame, (wrist_x, wrist_y), (elbow_x, elbow_y), (255, 255, 255), thickness=2)"
      ],
      "metadata": {
        "id": "DN1pvcTzz7A2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to blackout the background and keep only the hand movements\n",
        "def blackout_background(video_path, output_path):\n",
        "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video details\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    # Video writer to save the processed video\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert the image to RGB as MediaPipe expects this format\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get hand landmarks\n",
        "        result = hands.process(image_rgb)\n",
        "\n",
        "        # Create a blackout frame (all black)\n",
        "        black_frame = np.zeros_like(frame)\n",
        "\n",
        "        if result.multi_hand_landmarks:\n",
        "            for hand_landmarks in result.multi_hand_landmarks:\n",
        "                # Draw the hand landmarks on the black frame\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    black_frame,\n",
        "                    hand_landmarks,\n",
        "                    mp_hands.HAND_CONNECTIONS,\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2)\n",
        "                )\n",
        "                # Extend the wrist to elbow\n",
        "                extend_wrist_to_elbow(black_frame, hand_landmarks)\n",
        "\n",
        "        # Write the black_frame with only hands visible to output video\n",
        "        out.write(black_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    hands.close()"
      ],
      "metadata": {
        "id": "lo9V7TQLz23j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: Process the dictionary videos\n",
        "dictionary_videos_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/Words\"  # Replace with actual path\n",
        "blackout_videos_output_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black1\"  # Replace with actual output folder\n",
        "\n",
        "# Iterate over all dictionary videos and process each\n",
        "for video_file in os.listdir(dictionary_videos_path):\n",
        "    if video_file.endswith(\".mp4\"):  # Process only .mp4 videos (adjust if necessary)\n",
        "        video_path = os.path.join(dictionary_videos_path, video_file)\n",
        "        output_path = os.path.join(blackout_videos_output_path, video_file)\n",
        "        blackout_background(video_path, output_path)\n",
        "\n",
        "print(\"All dictionary videos processed and saved in the blackout dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQqI-nJqtevF",
        "outputId": "0f6b4a67-43a2-431b-e1c0-a3d51d190c1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dictionary videos processed and saved in the blackout dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IChyeCUEtext"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# with face"
      ],
      "metadata": {
        "id": "Zr2xL_n004aN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "qMUXKZRe085Z"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize MediaPipe Hands and Face Mesh models\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_face_mesh = mp.solutions.face_mesh"
      ],
      "metadata": {
        "id": "FpGkMQGP0-Nf"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extend wrist to elbow approximation\n",
        "def extend_wrist_to_elbow(black_frame, hand_landmarks):\n",
        "    # Get wrist landmark (landmark 0 in MediaPipe's hand model)\n",
        "    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
        "\n",
        "    # Calculate the elbow position by extending from wrist along the vector\n",
        "    wrist_x = int(wrist.x * black_frame.shape[1])\n",
        "    wrist_y = int(wrist.y * black_frame.shape[0])\n",
        "\n",
        "    # Arbitrary length to extend for the 'elbow'\n",
        "    length = 100  # You can adjust this based on how long you want the arm to be\n",
        "\n",
        "    # Calculate 'elbow' position\n",
        "    elbow_x = wrist_x\n",
        "    elbow_y = wrist_y + length\n",
        "\n",
        "    # Draw the line from wrist to the 'elbow'\n",
        "    cv2.line(black_frame, (wrist_x, wrist_y), (elbow_x, elbow_y), (255, 255, 255), thickness=2)"
      ],
      "metadata": {
        "id": "VP7Bklm91Arj"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to blackout the background and keep only hand and face landmarks\n",
        "def blackout_background(video_path, output_path):\n",
        "    hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video details\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    # Video writer to save the processed video\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert the image to RGB as MediaPipe expects this format\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Get hand landmarks\n",
        "        result_hands = hands.process(image_rgb)\n",
        "\n",
        "        # Get face landmarks\n",
        "        result_face = face_mesh.process(image_rgb)\n",
        "\n",
        "        # Create a blackout frame (all black)\n",
        "        black_frame = np.zeros_like(frame)\n",
        "\n",
        "        # Draw hands if found\n",
        "        if result_hands.multi_hand_landmarks:\n",
        "            for hand_landmarks in result_hands.multi_hand_landmarks:\n",
        "                # Draw the hand landmarks on the black frame\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    black_frame,\n",
        "                    hand_landmarks,\n",
        "                    mp_hands.HAND_CONNECTIONS,\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2)\n",
        "                )\n",
        "                # Extend the wrist to elbow\n",
        "                extend_wrist_to_elbow(black_frame, hand_landmarks)\n",
        "\n",
        "        # Draw face if found\n",
        "        if result_face.multi_face_landmarks:\n",
        "            for face_landmarks in result_face.multi_face_landmarks:\n",
        "                mp_drawing.draw_landmarks(\n",
        "                    black_frame,\n",
        "                    face_landmarks,\n",
        "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1, circle_radius=1),\n",
        "                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1)\n",
        "                )\n",
        "\n",
        "        # Write the black_frame with only hands and face visible to output video\n",
        "        out.write(black_frame)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    hands.close()\n",
        "    face_mesh.close()"
      ],
      "metadata": {
        "id": "CEJa1vsTte0M"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage: Process the dictionary videos\n",
        "dictionary_videos_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/Words\"  # Replace with actual path\n",
        "blackout_videos_output_path = \"/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black1\"  # Replace with actual output folder\n",
        "\n",
        "# Iterate over all dictionary videos and process each\n",
        "for video_file in os.listdir(dictionary_videos_path):\n",
        "    if video_file.endswith(\".mp4\"):  # Process only .mp4 videos (adjust if necessary)\n",
        "        video_path = os.path.join(dictionary_videos_path, video_file)\n",
        "        output_path = os.path.join(blackout_videos_output_path, video_file)\n",
        "        blackout_background(video_path, output_path)\n",
        "\n",
        "print(\"All dictionary videos processed and saved in the blackout dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfvEWD_5te2u",
        "outputId": "649caf91-e787-4089-fcc9-44f3c2dfc843"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All dictionary videos processed and saved in the blackout dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3XDs9I8Yte5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# frame extraction"
      ],
      "metadata": {
        "id": "N-JNey9d1e8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Frame Extraction\n",
        "def extract_frames(video_path, frame_rate=1):\n",
        "    print(f\"Extracting frames from {video_path}...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % frame_rate == 0:\n",
        "            frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    print(f\"Extracted {len(frames)} frames from {video_path}.\")\n",
        "    return frames\n",
        "\n",
        "# Step 2: Normalization & Resizing\n",
        "def preprocess_frame(frame, size=(224, 224)):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    frame = transform(frame)\n",
        "    # print(\"Frame preprocessed.\")\n",
        "    return frame\n",
        "\n",
        "# Step 3: Augmentation (optional)\n",
        "def augment_frames(frames):\n",
        "    print(\"Augmenting frames...\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0))\n",
        "    ])\n",
        "    augmented_frames = []\n",
        "    for frame in frames:\n",
        "        frame = transform(frame)\n",
        "        augmented_frames.append(frame)\n",
        "    print(f\"Augmented {len(augmented_frames)} frames.\")\n",
        "    return augmented_frames\n",
        "\n",
        "# Step 4: Label Assignment\n",
        "def get_label_from_filename(filename):\n",
        "    label = filename.split()[1].split('.')[0]\n",
        "    print(f\"Assigned label '{label}' to {filename}.\")\n",
        "    return label\n",
        "\n",
        "# Main Preprocessing Function\n",
        "def preprocess_videos(directory):\n",
        "    print(f\"Preprocessing videos in directory: {directory}\")\n",
        "    video_data = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            video_path = os.path.join(directory, filename)\n",
        "            frames = extract_frames(video_path)\n",
        "            frames = [preprocess_frame(frame) for frame in frames]\n",
        "            frames = augment_frames(frames)  # Optional\n",
        "            label = get_label_from_filename(filename)\n",
        "            video_data.append(frames)\n",
        "            labels.append(label)\n",
        "    print(\"Preprocessing complete.\")\n",
        "    return video_data, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "HXsI5OZhte78"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Example usage\n",
        "video_directory = 'Data1/black1'\n",
        "video_data, labels = preprocess_videos(video_directory)\n",
        "\n",
        "# Save preprocessed data for later use\n",
        "torch.save(video_data, 'video_data.pt')\n",
        "torch.save(labels, 'labels.pt')\n",
        "print(\"Data saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CflP_wFgte-t",
        "outputId": "1f59c350-a2d9-4a09-d06f-c5cc61fcc10d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing videos in directory: Data1/black1\n",
            "Extracting frames from Data1/black1/a0 butcher.mp4...\n",
            "Extracted 58 frames from Data1/black1/a0 butcher.mp4.\n",
            "Augmenting frames...\n",
            "Augmented 58 frames.\n",
            "Assigned label 'butcher' to a0 butcher.mp4.\n",
            "Extracting frames from Data1/black1/a1 aadhar_card.mp4...\n",
            "Extracted 94 frames from Data1/black1/a1 aadhar_card.mp4.\n",
            "Augmenting frames...\n",
            "Augmented 94 frames.\n",
            "Assigned label 'aadhar_card' to a1 aadhar_card.mp4.\n",
            "Preprocessing complete.\n",
            "Data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# labelling"
      ],
      "metadata": {
        "id": "lGghiUQa2Her"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a label mapping\n",
        "unique_labels = list(set(labels))\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Convert string labels to numerical labels\n",
        "numerical_labels = [label_to_index[label] for label in labels]"
      ],
      "metadata": {
        "id": "V09VIwMktfCA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SignDataset(Dataset):\n",
        "    def __init__(self, video_data, labels, transform=None, max_frames=100):\n",
        "        self.video_data = video_data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.max_frames = max_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video = self.video_data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Pad or truncate video to max_frames\n",
        "        if len(video) < self.max_frames:\n",
        "            padding = [torch.zeros_like(video[0])] * (self.max_frames - len(video))\n",
        "            video.extend(padding)\n",
        "        else:\n",
        "            video = video[:self.max_frames]\n",
        "\n",
        "        if self.transform:\n",
        "            video = [self.transform(frame) for frame in video]\n",
        "        video = torch.stack(video)\n",
        "        label = torch.tensor(label, dtype=torch.long)  # Ensure label is a tensor of type long\n",
        "        return video, label"
      ],
      "metadata": {
        "id": "Tu4gUY9X2Kx1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WS4WByE5tfEb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pMQ6trY9tfHb"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model architecture"
      ],
      "metadata": {
        "id": "utuKAyal2QBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n"
      ],
      "metadata": {
        "id": "IxUbAMO5tfKA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SignRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SignRecognitionModel, self).__init__()\n",
        "        # Use a pre-trained 2D-CNN (e.g., ResNet) as the feature extractor\n",
        "        self.cnn = models.resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # LSTM to capture temporal relationships\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Forward pass started.\")\n",
        "        batch_size, seq_length, c, h, w = x.size()\n",
        "        cnn_out = []\n",
        "\n",
        "        # Pass each frame through the CNN\n",
        "        for t in range(seq_length):\n",
        "            frame = x[:, t, :, :, :]\n",
        "            cnn_out.append(self.cnn(frame))\n",
        "            print(f\"Processed frame {t+1}/{seq_length} through CNN.\")\n",
        "\n",
        "        # Stack the CNN outputs and pass through LSTM\n",
        "        cnn_out = torch.stack(cnn_out, dim=1)\n",
        "        print(\"Stacked CNN outputs.\")\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        print(\"Passed through LSTM.\")\n",
        "\n",
        "        # Take the output of the last LSTM cell\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        print(\"Extracted output from the last LSTM cell.\")\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        out = self.fc(lstm_out)\n",
        "        print(\"Passed through the fully connected layer.\")\n",
        "        print(\"Forward pass completed.\")\n",
        "        return out"
      ],
      "metadata": {
        "id": "nf-hQZY1tfMx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2z2-iNvCtfQL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4uFwuwN2kHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RjqnK_Gz2kJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# query processing"
      ],
      "metadata": {
        "id": "QseDg75r2lha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "spP9bHAd2kMQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load preprocessed data and labels\n",
        "video_data = torch.load('video_data.pt', weights_only=True)\n",
        "labels = torch.load('labels.pt', weights_only=True)\n",
        "\n",
        "print(\"Data and labels loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ_ULVpS2kO6",
        "outputId": "a1014422-a593-454a-c07f-fe392e31e9eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data and labels loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Function to perform exact match\n",
        "def exact_match(query, labels):\n",
        "    if query in labels:\n",
        "        index = labels.index(query)\n",
        "        return video_data[index]\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "f7CYJFnt2kRd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to get BERT embeddings for a word\n",
        "def get_bert_embedding(word):\n",
        "    inputs = tokenizer(word, return_tensors='pt')\n",
        "    outputs = bert_model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# Function to find the most similar word using BERT\n",
        "def similar_word_search(query, labels):\n",
        "    query_embedding = get_bert_embedding(query)\n",
        "    label_embeddings = [get_bert_embedding(label) for label in labels]\n",
        "\n",
        "    similarities = [cosine_similarity(query_embedding, label_embedding)[0][0] for label_embedding in label_embeddings]\n",
        "    max_similarity_index = np.argmax(similarities)\n",
        "\n",
        "    if similarities[max_similarity_index] > 0.7:  # Threshold for similarity\n",
        "        return labels[max_similarity_index]\n",
        "    else:\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "La3s2ex62kTz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the labels from the .pt file\n",
        "labels_path = \"labels.pt\"  # Replace with the actual path to labels.pt\n",
        "labels = torch.load(labels_path)\n",
        "\n",
        "# Print the loaded labels\n",
        "print(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIKf7rXhEBiS",
        "outputId": "97bce3c6-a044-473a-e48c-1bf43f435acd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['butcher', 'aadhar_card']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-086fb1bc4ec2>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  labels = torch.load(labels_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import BertTokenizer, BertModel\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import numpy as np\n",
        "\n",
        "# # Load BERT tokenizer and model\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Function to get BERT embeddings for a word\n",
        "# def get_bert_embedding(word):\n",
        "#     inputs = tokenizer(word, return_tensors='pt')\n",
        "#     outputs = bert_model(**inputs)\n",
        "#     return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "# # Function to find exact match for the word in the dictionary\n",
        "# def exact_match(query, labels):\n",
        "#     if query in labels:\n",
        "#         # Return corresponding video frames for the exact match\n",
        "#         return query  # Placeholder for actual frame retrieval logic\n",
        "#     return None\n",
        "\n",
        "# # Function to search for similar words using BERT embeddings\n",
        "# def similar_word_search(query, labels):\n",
        "#     query_embedding = get_bert_embedding(query)\n",
        "\n",
        "#     # Find the embeddings for all dictionary labels\n",
        "#     label_embeddings = {label: get_bert_embedding(label) for label in labels}\n",
        "\n",
        "#     # Calculate cosine similarities between the query and each label\n",
        "#     similarities = {label: cosine_similarity(query_embedding, label_embedding)[0][0]\n",
        "#                     for label, label_embedding in label_embeddings.items()}\n",
        "\n",
        "#     # Find the most similar word based on cosine similarity\n",
        "#     similar_word = max(similarities, key=similarities.get)\n",
        "\n",
        "#     # Set a threshold for similarity to ensure the words are closely related\n",
        "#     if similarities[similar_word] > 0.7:  # Adjust threshold if necessary\n",
        "#         return similar_word\n",
        "#     return None\n",
        "\n",
        "# # Example usage\n",
        "# query = \"butcher\"\n",
        "# labels = [\"chef\", \"cook\", \"carpenter\", \"butcher\"]  # Example dictionary of words\n",
        "\n",
        "query = \"butcher\"\n",
        "\n",
        "video_frames = exact_match(query, labels)\n",
        "\n",
        "if not video_frames:\n",
        "    print(f\"Exact match not found for '{query}'. Searching for similar words...\")\n",
        "    similar_word = similar_word_search(query, labels)\n",
        "    if similar_word:\n",
        "        video_frames = exact_match(similar_word, labels)\n",
        "        print(f\"Similar word found: '{similar_word}', corresponding video frames retrieved.\")\n",
        "    else:\n",
        "        print(\"No similar word found.\")\n",
        "else:\n",
        "    print(f\"Exact match found for '{query}', corresponding video frames retrieved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjnH2bNO2kcb",
        "outputId": "3c7b51d8-30c4-4ac9-fab3-3058ac43ae14"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match found for 'butcher', corresponding video frames retrieved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_frames(frames, model, max_frames=100):\n",
        "    preprocessed_frames = [preprocess_frame(frame) for frame in frames]\n",
        "\n",
        "    # Pad or truncate frames to max_frames\n",
        "    if len(preprocessed_frames) < max_frames:\n",
        "        padding = [torch.zeros_like(preprocessed_frames[0])] * (max_frames - len(preprocessed_frames))\n",
        "        preprocessed_frames.extend(padding)\n",
        "    else:\n",
        "        preprocessed_frames = preprocessed_frames[:max_frames]\n",
        "\n",
        "    preprocessed_frames = torch.stack(preprocessed_frames).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(preprocessed_frames)\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "FEYhcoiB2ke1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LQOOlbhL3weQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Bkc8TnW3wg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l8bH5p7m3wjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gK3WsH2V3wnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature extraction forom retrived video"
      ],
      "metadata": {
        "id": "o4IZ_smJ3xEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n"
      ],
      "metadata": {
        "id": "HFL04Htw2khh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SignRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SignRecognitionModel, self).__init__()\n",
        "        # Use a pre-trained 2D-CNN (e.g., ResNet) as the feature extractor\n",
        "        self.cnn = models.resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # LSTM to capture temporal relationships\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length, c, h, w = x.size()\n",
        "        cnn_out = []\n",
        "\n",
        "        # Pass each frame through the CNN\n",
        "        for t in range(seq_length):\n",
        "            frame = x[:, t, :, :, :]\n",
        "            cnn_out.append(self.cnn(frame))\n",
        "\n",
        "        # Stack the CNN outputs and pass through LSTM\n",
        "        cnn_out = torch.stack(cnn_out, dim=1)\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "\n",
        "        # Take the output of the last LSTM cell\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "uToeV_3R2kj9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "# import the necessary module for DataLoader\n",
        "from torch.utils.data import DataLoader # import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# Prepare Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "max_frames = 100  # Set the maximum number of frames for each video\n",
        "dataset = SignDataset(video_data, numerical_labels, transform=transform, max_frames=max_frames)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "-VBr4weI2kmz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "num_classes = len(unique_labels)\n",
        "model = SignRecognitionModel(num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Create the directory to save the model if it doesn't exist\n",
        "save_dir = '/content/trialreu1'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training loop with model saving at the end of training\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=2, save_path=None):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Batch {batch_idx}/{len(dataloader)} processed. Current loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} completed. Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save the trained model at the specified path\n",
        "    if save_path:\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# Set the save path to the content/model folder\n",
        "save_path = os.path.join(save_dir, 'sign_recognition_model1.pth')\n",
        "\n",
        "# Train the model and save it\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs=2, save_path=save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwzPGaJT2kqO",
        "outputId": "ba9ed41a-b9cc-4f42-a9af-dfcbb51affc7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 115MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/2\n",
            "Batch 0/1 processed. Current loss: 0.6886\n",
            "Epoch 1/2 completed. Loss: 0.6886\n",
            "Starting epoch 2/2\n",
            "Batch 0/1 processed. Current loss: 0.5480\n",
            "Epoch 2/2 completed. Loss: 0.5480\n",
            "Model saved to /content/trialreu1/sign_recognition_model1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knn2a_E7GmEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5FWwM4rwJBwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aN4YTt0GGmG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7PPzhJBRD06"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Spotting in Continuous Video (Temporal Localization)\n",
        "\n",
        "Summary\n",
        "\n",
        "Input Continuous Video: Extract frames and preprocess them.\n",
        "\n",
        "Feature Extraction: Use the same feature extractor (CNN+LSTM) to extract features from the continuous video.\n",
        "\n",
        "Sliding Window Approach: Slide a window over the continuous video to extract features for each segment.\n",
        "\n",
        "Similarity Search: Compute similarity scores between the segment features and the retrieved video's features.\n",
        "\n",
        "Localization: Predict the time interval in the continuous video where the sign appears."
      ],
      "metadata": {
        "id": "NkWGycE1RHMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# MediaPipe setup\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_face = mp.solutions.face_mesh\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
        "face_mesh = mp_face.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
        "\n",
        "# Function to blackout background and keep only hands and face\n",
        "def blackout_background_with_hands_and_face(frame):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results_hands = hands.process(frame_rgb)\n",
        "    results_face = face_mesh.process(frame_rgb)\n",
        "\n",
        "    # Create an all-black frame of the same size\n",
        "    black_frame = np.zeros_like(frame)\n",
        "\n",
        "    # Draw hand landmarks on the black frame\n",
        "    if results_hands.multi_hand_landmarks:\n",
        "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
        "            mp.solutions.drawing_utils.draw_landmarks(\n",
        "                black_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
        "\n",
        "    # Draw face landmarks on the black frame\n",
        "    if results_face.multi_face_landmarks:\n",
        "        for face_landmarks in results_face.multi_face_landmarks:\n",
        "            mp.solutions.drawing_utils.draw_landmarks(\n",
        "                black_frame, face_landmarks, mp_face.FACEMESH_CONTOURS)\n",
        "\n",
        "    return black_frame\n",
        "\n",
        "# Step 1: Input Continuous Video - Extract frames from the continuous video, apply blackout, and save the video\n",
        "def extract_and_blackout_frames(video_path, output_path, frame_rate=1):\n",
        "    print(f\"Extracting and processing frames from {video_path}...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get frame dimensions\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Get the original video's FPS\n",
        "\n",
        "    # Set up VideoWriter to save the processed frames into a video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % frame_rate == 0:\n",
        "            # Blackout background for the current frame (keep only hands and face)\n",
        "            processed_frame = blackout_background_with_hands_and_face(frame)\n",
        "            out.write(processed_frame)  # Write the processed frame to the video\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Extracted and processed frames saved to {output_path}.\")\n",
        "\n",
        "# Example usage:\n",
        "video_path = '/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/v0.mp4'\n",
        "output_path = '/content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black.mp4'\n",
        "extract_and_blackout_frames(video_path, output_path, frame_rate=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr1VrBlWRCbi",
        "outputId": "4d036225-6870-4204-c5fd-ac1fb653a2ab"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting and processing frames from /content/drive/MyDrive/REU/reuwork/trialreu1/Data1/v0.mp4...\n",
            "Extracted and processed frames saved to /content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black.mp4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Feature Extraction\n",
        "# Use the same feature extractor (CNN+LSTM) to extract features from the continuous video.\n",
        "\n",
        "def extract_features_from_video(video_path, model, max_frames=100):\n",
        "    frames = extract_frames(video_path)\n",
        "    preprocessed_frames = [preprocess_frame(frame) for frame in frames]\n",
        "\n",
        "    # Pad or truncate frames to max_frames\n",
        "    if len(preprocessed_frames) < max_frames:\n",
        "        padding = [torch.zeros_like(preprocessed_frames[0])] * (max_frames - len(preprocessed_frames))\n",
        "        preprocessed_frames.extend(padding)\n",
        "    else:\n",
        "        preprocessed_frames = preprocessed_frames[:max_frames]\n",
        "\n",
        "    preprocessed_frames = torch.stack(preprocessed_frames).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        features = model(preprocessed_frames)\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "3WX5BJQFJDXW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Sliding Window Approach\n",
        "# Slide a window over the continuous video to extract features for each segment.\n",
        "def sliding_window_features(video_path, model, window_size=30, stride=15):\n",
        "    frames = extract_frames(video_path)\n",
        "    preprocessed_frames = [preprocess_frame(frame) for frame in frames]\n",
        "    num_frames = len(preprocessed_frames)\n",
        "\n",
        "    window_features = []\n",
        "    for start in range(0, num_frames - window_size + 1, stride):\n",
        "        window = preprocessed_frames[start:start + window_size]\n",
        "        window = torch.stack(window).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model(window)\n",
        "\n",
        "        window_features.append((start, features))\n",
        "\n",
        "    return window_features\n"
      ],
      "metadata": {
        "id": "_6tCXvN3GmKd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Similarity Search\n",
        "# Compute similarity scores between the segment features and the retrieved video's features.\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def compute_similarity(retrieved_features, window_features):\n",
        "    similarities = []\n",
        "    for start, features in window_features:\n",
        "        similarity = cosine_similarity(retrieved_features, features)\n",
        "        similarities.append((start, similarity))\n",
        "    return similarities"
      ],
      "metadata": {
        "id": "VzLfhV_pRgfl"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Localization\n",
        "# Predict the time interval in the continuous video where the sign appears based on the similarity scores.\n",
        "\n",
        "def find_best_match(similarities, threshold=0.8):\n",
        "    best_match = None\n",
        "    best_score = -1\n",
        "    for start, similarity in similarities:\n",
        "        score = similarity[0][0]\n",
        "        if score > best_score and score > threshold:\n",
        "            best_score = score\n",
        "            best_match = start\n",
        "    return best_match, best_score"
      ],
      "metadata": {
        "id": "bs7_f_cMRiOo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "continuous_video_path = output_path\n",
        "\n",
        "# The video is now saved as 'blackcont.mp4' with only hands and face visible.\n",
        "\n",
        "# Extract features from the retrieved video frames\n",
        "if video_frames:\n",
        "    retrieved_features = extract_features_from_frames(video_frames, model)\n",
        "    if retrieved_features is None:\n",
        "        print(\"Failed to extract features from the retrieved video frames.\")\n",
        "    else:\n",
        "        # Extract features from the continuous video using sliding window\n",
        "        window_features = sliding_window_features(continuous_video_path, model)\n",
        "\n",
        "        # Compute similarity scores\n",
        "        similarities = compute_similarity(retrieved_features, window_features)\n",
        "\n",
        "        # Find the best match\n",
        "        best_match, best_score = find_best_match(similarities)\n",
        "\n",
        "        if best_match is not None:\n",
        "            print(f\"Sign found at timestamp: {best_match} frames with similarity score: {best_score:.4f}\")\n",
        "        else:\n",
        "            print(\"Sign not found in the continuous video.\")\n",
        "else:\n",
        "    print(\"No video frames retrieved for the query.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAr5zb0iRjsF",
        "outputId": "08eb3278-769c-4dfe-ae7c-36298548ea66"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting frames from /content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black.mp4...\n",
            "Extracted 149 frames from /content/drive/MyDrive/REU/reuwork/trialreu1/Data1/black.mp4.\n",
            "Sign not found in the continuous video.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JbErUYdxtSkn"
      }
    }
  ]
}