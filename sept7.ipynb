{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khushi-MA/trialreu1/blob/main/sept7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-abzVzDFWits",
        "outputId": "eeb8993d-4504-46fa-c3d9-9697e40241bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'trialreu1'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 3 (delta 0), pack-reused 15 (from 1)\u001b[K\n",
            "Receiving objects: 100% (21/21), 78.06 MiB | 26.07 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Khushi-MA/trialreu1.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtXSwRzZYVul",
        "outputId": "4518223c-d908-4f1c-c3c4-48d0a6b0758e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcUbBJ7OWoWg",
        "outputId": "7a6c2855-e38e-4055-9b6f-f39938731d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/trialreu1\n",
            "/content/trialreu1\n",
            "Data1  main.ipynb  README.md  sept7.ipynb\n"
          ]
        }
      ],
      "source": [
        "%cd trialreu1\n",
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rpt6C_ooZ3S8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVuaGLIeZ3Xs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "45HYA30xZ3eq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xaa3JC6kXyoH"
      },
      "outputs": [],
      "source": [
        "# Step 1: Frame Extraction\n",
        "def extract_frames(video_path, frame_rate=1):\n",
        "    print(f\"Extracting frames from {video_path}...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % frame_rate == 0:\n",
        "            frames.append(frame)\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    print(f\"Extracted {len(frames)} frames from {video_path}.\")\n",
        "    return frames\n",
        "\n",
        "# Step 2: Normalization & Resizing\n",
        "def preprocess_frame(frame, size=(224, 224)):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    frame = transform(frame)\n",
        "    # print(\"Frame preprocessed.\")\n",
        "    return frame\n",
        "\n",
        "# Step 3: Augmentation (optional)\n",
        "def augment_frames(frames):\n",
        "    print(\"Augmenting frames...\")\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0))\n",
        "    ])\n",
        "    augmented_frames = []\n",
        "    for frame in frames:\n",
        "        frame = transform(frame)\n",
        "        augmented_frames.append(frame)\n",
        "    print(f\"Augmented {len(augmented_frames)} frames.\")\n",
        "    return augmented_frames\n",
        "\n",
        "# Step 4: Label Assignment\n",
        "def get_label_from_filename(filename):\n",
        "    label = filename.split()[1].split('.')[0]\n",
        "    print(f\"Assigned label '{label}' to {filename}.\")\n",
        "    return label\n",
        "\n",
        "# Main Preprocessing Function\n",
        "def preprocess_videos(directory):\n",
        "    print(f\"Preprocessing videos in directory: {directory}\")\n",
        "    video_data = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            video_path = os.path.join(directory, filename)\n",
        "            frames = extract_frames(video_path)\n",
        "            frames = [preprocess_frame(frame) for frame in frames]\n",
        "            frames = augment_frames(frames)  # Optional\n",
        "            label = get_label_from_filename(filename)\n",
        "            video_data.append(frames)\n",
        "            labels.append(label)\n",
        "    print(\"Preprocessing complete.\")\n",
        "    return video_data, labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXb6fLzCX0L5",
        "outputId": "dec6a77f-3e30-4c4b-9a13-f92979b9c8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing videos in directory: Data1/Words/\n",
            "Extracting frames from Data1/Words/a0 butcher.mp4...\n",
            "Extracted 58 frames from Data1/Words/a0 butcher.mp4.\n",
            "Augmenting frames...\n",
            "Augmented 58 frames.\n",
            "Assigned label 'butcher' to a0 butcher.mp4.\n",
            "Extracting frames from Data1/Words/a1 aadhar_card.mp4...\n",
            "Extracted 94 frames from Data1/Words/a1 aadhar_card.mp4.\n",
            "Augmenting frames...\n",
            "Augmented 94 frames.\n",
            "Assigned label 'aadhar_card' to a1 aadhar_card.mp4.\n",
            "Extracting frames from Data1/Words/a3 dedication.mp4...\n",
            "Extracted 61 frames from Data1/Words/a3 dedication.mp4.\n",
            "Augmenting frames...\n",
            "Augmented 61 frames.\n",
            "Assigned label 'dedication' to a3 dedication.mp4.\n",
            "Extracting frames from Data1/Words/a2 cinematography.mp4...\n",
            "Extracted 72 frames from Data1/Words/a2 cinematography.mp4.\n",
            "Augmenting frames...\n",
            "Augmented 72 frames.\n",
            "Assigned label 'cinematography' to a2 cinematography.mp4.\n",
            "Preprocessing complete.\n",
            "Data saved.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "video_directory = 'Data1/Words/'\n",
        "video_data, labels = preprocess_videos(video_directory)\n",
        "\n",
        "# Save preprocessed data for later use\n",
        "torch.save(video_data, 'video_data.pt')\n",
        "torch.save(labels, 'labels.pt')\n",
        "print(\"Data saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r40ulAIaaMRd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4VNdoFZwr3B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08hZOIjMwr5O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owP3zt-8wr77"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuN5JDzhwr-c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zy5Db_QX0PA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX4yVOWLl--h"
      },
      "source": [
        "# 2. Model Architecture (Isolated Sign Recognition):\n",
        "This model will be trained on your isolated dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3qyf9XySl-Td"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "viHHrNO6X0RY"
      },
      "outputs": [],
      "source": [
        "class SignRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SignRecognitionModel, self).__init__()\n",
        "        # Use a pre-trained 2D-CNN (e.g., ResNet) as the feature extractor\n",
        "        self.cnn = models.resnet18(pretrained=True)\n",
        "        self.cnn.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "        # LSTM to capture temporal relationships\n",
        "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Forward pass started.\")\n",
        "        batch_size, seq_length, c, h, w = x.size()\n",
        "        cnn_out = []\n",
        "\n",
        "        # Pass each frame through the CNN\n",
        "        for t in range(seq_length):\n",
        "            frame = x[:, t, :, :, :]\n",
        "            cnn_out.append(self.cnn(frame))\n",
        "            print(f\"Processed frame {t+1}/{seq_length} through CNN.\")\n",
        "\n",
        "        # Stack the CNN outputs and pass through LSTM\n",
        "        cnn_out = torch.stack(cnn_out, dim=1)\n",
        "        print(\"Stacked CNN outputs.\")\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        print(\"Passed through LSTM.\")\n",
        "\n",
        "        # Take the output of the last LSTM cell\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        print(\"Extracted output from the last LSTM cell.\")\n",
        "\n",
        "        # Pass through the fully connected layer\n",
        "        out = self.fc(lstm_out)\n",
        "        print(\"Passed through the fully connected layer.\")\n",
        "        print(\"Forward pass completed.\")\n",
        "        return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTlfm0elu2Ht",
        "outputId": "309899c1-4630-4bd0-bb0a-94ad57812b2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 79.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example usage\n",
        "num_classes = 4  # Replace with the actual number of classes\n",
        "model = SignRecognitionModel(num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WQMKfJIEu4im"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nqEe4jfsX0UL"
      },
      "outputs": [],
      "source": [
        "# Example training loop\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "# Assuming you have a DataLoader `dataloader` for your dataset\n",
        "# train_model(model, dataloader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blh1sHycX0Wz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJCEWwfpX0Zd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P2aXxwwwtoJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IAiuuwqwtqy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsSB7X--wtti"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeL2p59xwt4X"
      },
      "source": [
        "# 3. Query Processing (Text Matching):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM5NoON1w9bn"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNx2BUtgw_4I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TafkidwkxAjm"
      },
      "source": [
        "# search exact word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8_c0fvU4w0iH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ8qHwDLxKXa",
        "outputId": "4de0161e-9193-40ef-9f93-e8a448b46c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data and labels loaded.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Load preprocessed data and labels\n",
        "video_data = torch.load('video_data.pt', weights_only=True)\n",
        "labels = torch.load('labels.pt', weights_only=True)\n",
        "\n",
        "print(\"Data and labels loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhJqcuvk2MFM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LEvAWUG6xQdN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Function to perform exact match\n",
        "def exact_match(query, labels):\n",
        "    if query in labels:\n",
        "        index = labels.index(query)\n",
        "        return video_data[index]\n",
        "    else:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69bktXygzlC_",
        "outputId": "4b768ca6-5ff5-44cc-ea4a-143a2e294afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact match found for 'butcher'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example usage\n",
        "query = \"butcher\"\n",
        "video_frames = exact_match(query, labels)\n",
        "if video_frames:\n",
        "    print(f\"Exact match found for '{query}'.\")\n",
        "else:\n",
        "    print(\"Exact match not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vKmKIR7zmEg",
        "outputId": "37281cdf-a19f-45d8-e4fa-6a44bcb23fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact match not found for '{query}'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example usage\n",
        "query = \"camerawork\"\n",
        "video_frames = exact_match(query, labels)\n",
        "if video_frames:\n",
        "    print(f\"Exact match found for '{query}'.\")\n",
        "else:\n",
        "    print(\"Exact match not found for '{query}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COdFRWkyycZl"
      },
      "source": [
        "# similar search (word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE2trstHzkLh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVGvPfi2xMQ3"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7ylw8RQ3WZQ",
        "outputId": "8401b00d-9597-4d13-dbb9-810dbbb988e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERT model and tokenizer loaded.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', clean_up_tokenization_spaces=True)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(\"BERT model and tokenizer loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpLEay_dyjrr",
        "outputId": "210a54d0-182c-48ad-f0c3-6f510054df37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvRwKi5Dyk3x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to get BERT embeddings for a word\n",
        "def get_bert_embedding(word):\n",
        "    inputs = tokenizer(word, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bASb2_tcyinH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to find the most similar word using BERT\n",
        "def similar_word_search(query, labels):\n",
        "    query_embedding = get_bert_embedding(query)\n",
        "    label_embeddings = [get_bert_embedding(label) for label in labels]\n",
        "\n",
        "    similarities = [cosine_similarity(query_embedding, label_embedding)[0][0] for label_embedding in label_embeddings]\n",
        "    max_similarity_index = np.argmax(similarities)\n",
        "\n",
        "    if similarities[max_similarity_index] > 0.7:  # Threshold for similarity\n",
        "        return labels[max_similarity_index]\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrPYIDoRw4nt"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage 1\n",
        "query = \"commitment\"\n",
        "\n",
        "if not video_frames:\n",
        "    similar_word = similar_word_search(query, labels)\n",
        "    if similar_word:\n",
        "        video_frames = exact_match(similar_word, labels)\n",
        "        print(f\"Similar word found: '{query}', corresponding video frames retrieved.\")\n",
        "    else:\n",
        "        print(\"No similar word found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS5pITVaw4qp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage 2\n",
        "if not video_frames:\n",
        "    similar_word = similar_word_search(query, labels)\n",
        "    if similar_word:\n",
        "        video_frames = exact_match(similar_word, labels)\n",
        "        print(f\"Similar word found: '{ocean}', corresponding video frames retrieved.\")\n",
        "    else:\n",
        "        print(\"No similar word found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hYnGFFyzHZ3",
        "outputId": "55781f6a-4604-4865-908b-e641de6fa409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dKNEix04L3_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r59OQGTX5JwM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44pjoMYfX9hq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvSQ_WdNYuk6"
      },
      "source": [
        "# . Feature Extraction from Retrieved Video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpImTmpZX9jy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiLWy3WFX9mN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BePsRREjX9o6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJim3Jt3X9re"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YaVEkwAX9uE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTuhqZzQX9wO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1hkX6_oX9yo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v73S4ozvX91E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-JrtVFo5Klw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyORgVL8/3pnLFeE8MpjVCSb",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "reutrial1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
